\chapter[Lec 1:]{Likelihood Equation: Regular Exponential Family}


% See \cref{sec:reg exp fam} for definition of the regular exponential family. 
It just gets repeated here by Geoff.

% \bigskip

% We assume the \hyperref[sec:reg conds I]{regularity conditions} to make it easy for us to apply all of these wonderfully and almost-too-convenient results (previously mentioned \hyperref[not thm:reg conds I]{here} for the single variable case):

% \begin{theorem}[Score statistic equals zero]\label{thm:score stat equals zero} 
%     \begin{equation}\label{eq:score stat equals zero}
%         \frac{\partial \log L(\bm{\theta})}{\partial \bm{\theta}} = \bm{0}.
%     \end{equation}
% \end{theorem}

% Note that \(\frac{\partial}{\partial \bm{\theta}}\) can be thought of as the differential operator \(\bm{\nabla}\), i.e. 
% \[\frac{\partial}{\partial \bm{\theta}} = \bm{\nabla} = \left(\frac{\partial}{\partial \theta_1},...,\frac{\partial}{\partial \theta_d}\right)^T,\]
% where \(d\) is the dimension of \bm{\theta} as stated in the definition of the regular exponential family. 

% \bigskip

% The proof hinges upon the idea that if have some function \(f\), in this case \(L(\bm{\theta})\), if the derivative \(f'\) is considered as \(\frac{f'\cdot f}{f}\), we can use the fact that \(\)